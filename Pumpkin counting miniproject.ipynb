{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rasterio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PUMPKIN_WIDTH = 3\n",
    "MIN_PUMPKIN_HEIGHT = 3\n",
    "FILTER = \"Gaussian\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Colour based segmentation\n",
    "This part consists of using colour based information\n",
    "to segment individual images from a pumpkin field.\n",
    "The segmented images should end up having a black\n",
    "background with smaller white objects on top."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.1\n",
    "Annotate some pumpkins in a test image and extract information about the average pumpkin colour\n",
    "in the annotated pixels. Calculate both mean value\n",
    "and standard variation. Use the following two colour\n",
    "spaces: RGB and CieLAB. Finally try to visualise\n",
    "the distribution of colour values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean color values of the annotated pixels:\n",
      " [[ 46.41416428]\n",
      " [150.11533625]\n",
      " [230.19802827]]\n",
      "Standard deviation of color values of the annotated pixels:\n",
      " [[21.71128969]\n",
      " [16.11388805]\n",
      " [17.11028936]]\n",
      "Covariance matrix of color values of the annotated pixels:\n",
      " [[471.40381144 205.71747089 131.0308569 ]\n",
      " [205.71747089 259.6704494  231.02876867]\n",
      " [131.0308569  231.02876867 292.77672851]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_true = cv2.imread(\"Output/1/1/BGR/Ground_true_EB-02-660_0594_0326.JPG\")\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0326.JPG')\n",
    "cv2.imwrite(\"Output/1/1/BGR/img.png\", img)\n",
    "\n",
    "# lower bound and upper bound for red color\n",
    "lower_bound = np.array([0, 0, 250])\n",
    "upper_bound = np.array([0, 0, 255])\n",
    "\n",
    "# Find the colors within the boundaries\n",
    "mask = cv2.inRange(img_true, lower_bound, upper_bound)\n",
    "cv2.imwrite(\"output/1/1/BGR/mask.png\", mask)\n",
    "mask_pixels = np.reshape(mask, (-1))\n",
    "\n",
    "img = cv2.bitwise_and(img, img, mask=mask)\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cv2.imwrite(\"output/1/1/BGR/RGB.png\", img)\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean, std = cv2.meanStdDev(img, mask=mask)\n",
    "print(f\"Mean color values of the annotated pixels:\\n {mean}\")\n",
    "print(f\"Standard deviation of color values of the annotated pixels:\\n {std}\")\n",
    "\n",
    "# Covariance\n",
    "cov = np.cov(pixels.transpose(), aweights=mask_pixels)\n",
    "print(f\"Covariance matrix of color values of the annotated pixels:\\n {cov}\")\n",
    "\n",
    "# Save the parameters\n",
    "np.savetxt(\"Output/1/1/BGR/lower.txt\", mean-std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/BGR/upper.txt\", mean+std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/BGR/mean.txt\", mean, delimiter=' ', fmt='%1.4f')\n",
    "\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "mask = cv2.inRange(img, mean-std, mean+std)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/1/BGR/test.png\", img_test)\n",
    "\n",
    "# Show the histogram\n",
    "colors = ['b', 'g', 'r']\n",
    "for dim in range(img_test.ndim):\n",
    "    data = np.reshape(img_test[:, :, dim], (-1))\n",
    "    plt.hist(data[data != 0], bins=255, color=colors[dim], alpha=0.5)\n",
    "plt.savefig(\"Output/1/1/BGR/histogram.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CieLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean color values of the annotated pixels:\n",
      " [[175.31995372]\n",
      " [150.28308435]\n",
      " [190.01353051]]\n",
      "Standard deviation of color values of the annotated pixels:\n",
      " [[14.39882563]\n",
      " [ 4.32889583]\n",
      " [ 7.2355212 ]]\n",
      "Covariance matrix of color values of the annotated pixels:\n",
      " [[207.33660837 -19.72096074  25.87952376]\n",
      " [-19.72096074  18.74028175   6.27398643]\n",
      " [ 25.87952376   6.27398643  52.35540042]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_true = cv2.imread('Output/1/1/CieLAB/Ground_true_EB-02-660_0594_0326.JPG')\n",
    "img = cv2.imread('Images/EB-02-660_0594_0326.JPG')\n",
    "\n",
    "# lower bound and upper bound for red color\n",
    "lower_bound = np.array([0, 0, 250])\n",
    "upper_bound = np.array([0, 0, 255])\n",
    "\n",
    "# Find the colors within the boundaries\n",
    "mask = cv2.inRange(img_true, lower_bound, upper_bound)\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/mask.png\", mask)\n",
    "mask_pixels = np.reshape(mask, (-1))\n",
    "\n",
    "img = cv2.bitwise_and(img, img, mask=mask)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/CieLAB.png\", img)\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean, std = cv2.meanStdDev(img, mask=mask)\n",
    "print(f\"Mean color values of the annotated pixels:\\n {mean}\")\n",
    "print(f\"Standard deviation of color values of the annotated pixels:\\n {std}\")\n",
    "\n",
    "# Covariance\n",
    "cov = np.cov(pixels.transpose(), aweights=mask_pixels)\n",
    "print(f\"Covariance matrix of color values of the annotated pixels:\\n {cov}\")\n",
    "\n",
    "# Save the parameters\n",
    "np.savetxt(\"Output/1/1/CieLAB/lower.txt\", mean-std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/CieLAB/upper.txt\", mean+std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/CieLAB/mean.txt\", mean, delimiter=' ', fmt='%1.4f')\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "mask = cv2.inRange(img, mean-std, mean+std)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/test.png\", img_test)\n",
    "\n",
    "# Show the histogram\n",
    "colors = ['b', 'g', 'r']\n",
    "for dim in range(img_test.ndim):\n",
    "    data = np.reshape(img_test[:, :, dim], (-1))\n",
    "    plt.hist(data[data != 0], bins=255, color=colors[dim], alpha=0.5)\n",
    "plt.savefig(\"Output/1/1/CieLAB/histogram.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.2\n",
    "Segment the orange pumpkins from the background\n",
    "using color information. Experiment with the following segmentation methods\n",
    "1. inRange with RGB values\n",
    "2. inRange with CieLAB values\n",
    "3. Distance in RGB space to a reference colour\n",
    "4. (HLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Test RGB\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "lower_bound = np.loadtxt(\"Output/1/1/BGR/lower.txt\", delimiter=' ')\n",
    "upper_bound = np.loadtxt(\"Output/1/1/BGR/upper.txt\", delimiter=' ')\n",
    "mask = cv2.inRange(img, lower_bound, upper_bound)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/2/BGR.png\", img_test)\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "lower_bound = np.loadtxt(f\"Output/1/1/CieLAB/lower.txt\", delimiter=' ')\n",
    "upper_bound = np.loadtxt(f\"Output/1/1/CieLAB/upper.txt\", delimiter=' ')\n",
    "mask = cv2.inRange(img, lower_bound, upper_bound)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/2/CieLAB.png\", img_test)\n",
    "\n",
    "\n",
    "# Distance in RGB space to a reference color\n",
    "ref = np.loadtxt(f\"Output/1/1/BGR/mean.txt\", delimiter=' ')\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cov = np.cov(pixels.transpose())\n",
    "diff = pixels - np.repeat([ref], np.ma.size(pixels,axis=0),axis=0)\n",
    "# Calculate the distances\n",
    "mahalanobis = np.sqrt(np.sum(np.dot(diff,np.linalg.inv(cov)) *diff , axis=1))  \n",
    "euclidian = np.sqrt(np.sum(diff * diff, axis=1))\n",
    "# Reshaping to image dimension\n",
    "mahalanobis_img = np.reshape(mahalanobis,(img.shape[0],img.shape[1]))\n",
    "euclidian_img = np.reshape(euclidian,(img.shape[0],img.shape[1]))\n",
    "# Scaling to 8 bit greyscale image\n",
    "scaled_mahalanobis_img = 255*mahalanobis_img/np.max(mahalanobis_img)\n",
    "scaled_euclidian_img = 255*euclidian_img/np.max(euclidian_img)\n",
    "# Saving scaled images\n",
    "cv2.imwrite(\"Output/1/2/euclidian_dist_map.png\", scaled_euclidian_img)\n",
    "cv2.imwrite(\"Output/1/2/mahalanobis_dist_map.png\", scaled_mahalanobis_img)\n",
    "\n",
    "# Setting up a threshhold for mahalanobis\n",
    "ret,threshold_img = cv2.threshold(scaled_mahalanobis_img,135,255, cv2.THRESH_BINARY_INV)\n",
    "threshold_img = threshold_img.astype(np.uint8)\n",
    "cv2.imwrite(\"Output/1/2/mahalanobis_segmented.png\", threshold_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.3\n",
    "Choose one segmentation method to use for the rest\n",
    "of the mini-project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "BGR = \"BGR\"\n",
    "CieLAB = \"CieLAB\"\n",
    "\n",
    "def segmentation(img: np.ndarray, method: str = CieLAB) -> np.ndarray:\n",
    "    \"\"\"Segmentation of an image to find a color spectrum\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "        type (str, optional): Type of image used for segmentation. Defaults to BGR.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image containing the orange color\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the lower and upper bounds\n",
    "    lower_bound = np.loadtxt(f\"Output/1/1/{method}/lower.txt\", delimiter=' ')\n",
    "    upper_bound = np.loadtxt(f\"Output/1/1/{method}/upper.txt\", delimiter=' ')\n",
    "    ref = np.loadtxt(f\"Output/1/1/{method}/mean.txt\", delimiter=' ')\n",
    "    # TODO\n",
    "    ref = np.loadtxt(f\"Output/1/1/BGR/mean.txt\", delimiter=' ')\n",
    "\n",
    "    # Copy the image\n",
    "    img_tmp = img.copy()\n",
    "\n",
    "    if method == BGR:\n",
    "        lower_bound = np.loadtxt(\"Output/1/1/BGR/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(\"Output/1/1/BGR/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "        return img_test\n",
    "    if method == CieLAB:\n",
    "        img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2LAB)\n",
    "        lower_bound = np.loadtxt(f\"Output/1/1/CieLAB/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(f\"Output/1/1/CieLAB/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "\n",
    "\n",
    "    # Distance in RGB space to a reference color\n",
    "    pixels = np.reshape(img_tmp, (-1, 3))\n",
    "    cov = np.cov(pixels.transpose())\n",
    "    diff = pixels - np.repeat([ref], np.ma.size(pixels,axis=0),axis=0)\n",
    "    # Calculate the distances\n",
    "    mahalanobis = np.sqrt(np.sum(np.dot(diff,np.linalg.inv(cov)) *diff , axis=1))  \n",
    "    # Reshaping to image dimension\n",
    "    mahalanobis_img = np.reshape(mahalanobis,(img_tmp.shape[0],img_tmp.shape[1]))\n",
    "    # Scaling to 8 bit greyscale image\n",
    "    scaled_mahalanobis_img = 255*mahalanobis_img/np.max(mahalanobis_img)\n",
    "    \n",
    "    # Setting up a threshhold for mahalanobis\n",
    "    ret,threshold_img = cv2.threshold(scaled_mahalanobis_img,135,255, cv2.THRESH_BINARY_INV)\n",
    "    return threshold_img.astype(np.uint8)\n",
    "\n",
    "# Test the segmentation\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img_test = segmentation(img, method=BGR)\n",
    "cv2.imwrite(\"Output/1/3/BGR.png\", img_test)\n",
    "img_test = segmentation(img, method= CieLAB)\n",
    "cv2.imwrite(\"Output/1/3/CieLAB.png\", img_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Counting objects\n",
    "This part is about counting objects in segmented\n",
    "images and then to generate some visual output that\n",
    "will help you to debug the programs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.1\n",
    "Count the number of orange blobs in the segmented\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number contours: 5378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "segmented = segmentation(img, method=CieLAB)\n",
    "contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Get the width, height and position of each contour\n",
    "width = []\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "        width.append(w)\n",
    "        cv2.circle(img, (x+int(w/2), y+int(h/2)), 10, (0, 0, 255), 2)\n",
    "\n",
    "# Print histogram of size\n",
    "plt.figure()\n",
    "plt.hist(width,bins=255)\n",
    "plt.savefig(\"Output/2/1/histogram.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(f\"Number contours: {len(contours)}\")\n",
    "cv2.imwrite(\"Output/2/1/circle_pumpkins.png\", img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.2\n",
    "Filter the segmented image to remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "segmented = segmentation(img, method=CieLAB)\n",
    "# Opening\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "filtered = cv2.erode(segmented, kernel, iterations=1)\n",
    "filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "cv2.imwrite(\"Output/2/2/filtered.png\", filtered)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.3\n",
    "Count the number of orange blobs in the filtered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number contours: 3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "filtered = cv2.imread('Output/2/2/filtered.png')\n",
    "filtered = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)\n",
    "contours, hierarchy = cv2.findContours(filtered, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Get the width, height and position of each contour\n",
    "width = []\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "        width.append(w)\n",
    "        cv2.circle(img, (x+int(w/2), y+int(h/2)), 10, (0, 0, 255), 2)\n",
    "\n",
    "# Print histogram of size\n",
    "plt.figure()\n",
    "plt.hist(width,bins=255)\n",
    "plt.savefig(\"Output/2/3/histogram.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Number contours: {len(contours)}\")\n",
    "cv2.imwrite(\"Output/2/3/circle_pumpkins.png\", img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.4\n",
    "Mark the located pumpkins in the input image. This\n",
    "step is for debugging purposes and to convince others\n",
    "that you have counted the pumpkins accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Generate an orthomosaic\n",
    "This part deals with orthorectifying multiple images\n",
    "of the same field into a single carthometric product.\n",
    "Choose proper settings for all below processes, taking into consideration the available computing resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.1\n",
    "Load data into Metashape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.2\n",
    "Perform bundle adjustment (align photos) and check\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.3\n",
    "Perform dense reconstruction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.4\n",
    "Create digital elevation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.5\n",
    "Create orthomosaic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.6\n",
    "Limit orthomosaic to pumpkin field"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Count in orthomosaic\n",
    "Use the python package rasterio to perform operations on the orthomosaic using a tile based approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.1\n",
    "Create code that only loads parts of the orthomosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "example.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mrasterio\\_base.pyx:308\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_base.pyx:219\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: example.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\SDU\\Semester 2\\Large-scale Drone Perception\\Large-scale_Drone_Perception\\Pumpkin counting miniproject.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrasterio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindows\u001b[39;00m \u001b[39mimport\u001b[39;00m Window\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexample.tif\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m rasterio\u001b[39m.\u001b[39;49mopen(filename) \u001b[39mas\u001b[39;00m src:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# tile ulc - upper left corner,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# lower left corner... and so on.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     window_location \u001b[39m=\u001b[39m Window\u001b[39m.\u001b[39mfrom_slices((tile\u001b[39m.\u001b[39mulc[\u001b[39m0\u001b[39m], tile\u001b[39m.\u001b[39mlrc[\u001b[39m0\u001b[39m]), (tile\u001b[39m.\u001b[39mulc[\u001b[39m1\u001b[39m], tile\u001b[39m.\u001b[39mlrc[\u001b[39m1\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kaspe/Documents/SDU/Semester%202/Large-scale%20Drone%20Perception/Large-scale_Drone_Perception/Pumpkin%20counting%20miniproject.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     img \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mread(window\u001b[39m=\u001b[39mwindow_location)\n",
      "File \u001b[1;32mc:\\Users\\kaspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rasterio\\env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     session \u001b[39m=\u001b[39m DummySession()\n\u001b[0;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m env_ctor(session\u001b[39m=\u001b[39msession):\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\kaspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rasterio\\__init__.py:321\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m path \u001b[39m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 321\u001b[0m     dataset \u001b[39m=\u001b[39m DatasetReader(path, driver\u001b[39m=\u001b[39mdriver, sharing\u001b[39m=\u001b[39msharing, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    323\u001b[0m     dataset \u001b[39m=\u001b[39m get_writer_for_path(path, driver\u001b[39m=\u001b[39mdriver)(\n\u001b[0;32m    324\u001b[0m         path, mode, driver\u001b[39m=\u001b[39mdriver, sharing\u001b[39m=\u001b[39msharing, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    325\u001b[0m     )\n",
      "File \u001b[1;32mrasterio\\_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: example.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "filename = \"example.tif\"\n",
    "\n",
    "def get_tiles(ds, width=256, height=256):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in  offsets:\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "with rasterio.open(filename) as src:\n",
    "    # tile ulc - upper left corner,\n",
    "    # lower left corner... and so on.\n",
    "    \n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(src):\n",
    "        rasterio.get_t\n",
    "        print(window)\n",
    "        meta['transform'] = transform\n",
    "        meta['width'], meta['height'] = window.width, window.height\n",
    "\n",
    "    window_location = Window.from_slices((tile.ulc[0], tile.lrc[0]), (tile.ulc[1], tile.lrc[1]))\n",
    "    img = src.read(window=window_location)\n",
    "\n",
    "    # Loaded image has a different shape\n",
    "    # than opencv image, so...\n",
    "    temp = img.transpose(1, 2, 0)\n",
    "    t2 = cv2.split(temp)\n",
    "    img_cv = cv2.merge([t2[2], t2[1], t2[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.2\n",
    "Design tile placement incl. overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.3\n",
    "Count pumpkins in each tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number contours: 3012\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def count_pumpkins(img: np.ndarray) -> int:\n",
    "    \"\"\"Count the number of pumpkins in the image\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "\n",
    "    Returns:\n",
    "        int: Number of pumpkins\n",
    "    \"\"\"\n",
    "    # Filter\n",
    "    if \"Open\" == FILTER:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        filtered = cv2.erode(img, kernel, iterations=1)\n",
    "        filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "    elif \"Gaussian\" == FILTER:\n",
    "        filtered = cv2.GaussianBlur(img, (3,3), cv2.BORDER_DEFAULT)\n",
    "    elif \"Median\" == FILTER:\n",
    "        filtered = cv2.medianBlur(img, 3)\n",
    "    else:\n",
    "        filtered = img\n",
    "    segmented = segmentation(filtered, method=CieLAB)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    pumpkins:int = 0\n",
    "    for c in contours:\n",
    "        _,_,w,h = cv2.boundingRect(c)\n",
    "        if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "            pumpkins += 1\n",
    "    return pumpkins\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "pumpkins = count_pumpkins(img)\n",
    "print(f\"Number contours: {pumpkins}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.4\n",
    "Deal with pumpkins in the overlap, so they are only\n",
    "counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.5\n",
    "Determine amount of pumpkins in the entire field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Endnotes\n",
    "Reflect on the conducted work in this miniproject."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5.1\n",
    "Determine GSD and size of the image field. What is\n",
    "the average number of pumpkins per area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5.2\n",
    "Reflect on whether the developed system is ready to\n",
    "help a farmer with the task of estimating the number\n",
    "of pumpkins in a field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
