{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rasterio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PUMPKIN_WIDTH = 3\n",
    "MIN_PUMPKIN_HEIGHT = 3\n",
    "FILTER = \"Gaussian\"\n",
    "\n",
    "WINDOW_WIDTH = 3000\n",
    "WINDOW_HEIGTH = 3000\n",
    "OVERLAP_WINDOW = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "BGR = \"BGR\"\n",
    "CieLAB = \"CieLAB\"\n",
    "\n",
    "def segmentation(img: np.ndarray, method: str = CieLAB) -> np.ndarray:\n",
    "    \"\"\"Segmentation of an image to find a color spectrum\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "        type (str, optional): Type of image used for segmentation. Defaults to BGR.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image containing the orange color\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the lower and upper bounds\n",
    "    lower_bound = np.loadtxt(f\"Output/1/1/{method}/lower.txt\", delimiter=' ')\n",
    "    upper_bound = np.loadtxt(f\"Output/1/1/{method}/upper.txt\", delimiter=' ')\n",
    "    ref = np.loadtxt(f\"Output/1/1/{method}/mean.txt\", delimiter=' ')\n",
    "    # TODO\n",
    "    ref = np.loadtxt(f\"Output/1/1/BGR/mean.txt\", delimiter=' ')\n",
    "\n",
    "    # Copy the image\n",
    "    img_tmp = img.copy()\n",
    "\n",
    "    if method == BGR:\n",
    "        lower_bound = np.loadtxt(\"Output/1/1/BGR/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(\"Output/1/1/BGR/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "        return img_test\n",
    "    if method == CieLAB:\n",
    "        img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2LAB)\n",
    "        lower_bound = np.loadtxt(f\"Output/1/1/CieLAB/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(f\"Output/1/1/CieLAB/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "\n",
    "\n",
    "    # Distance in RGB space to a reference color\n",
    "    pixels = np.reshape(img_tmp, (-1, 3))\n",
    "    cov = np.cov(pixels.transpose())\n",
    "    diff = pixels - np.repeat([ref], np.ma.size(pixels,axis=0),axis=0)\n",
    "    # Calculate the distances\n",
    "    mahalanobis = np.sqrt(np.sum(diff*diff , axis=1))  \n",
    "    # Reshaping to image dimension\n",
    "    mahalanobis_img = np.reshape(mahalanobis,(img_tmp.shape[0],img_tmp.shape[1]))\n",
    "    # Scaling to 8 bit greyscale image\n",
    "    scaled_mahalanobis_img = 255*mahalanobis_img/np.max(mahalanobis_img)\n",
    "    \n",
    "    # Setting up a threshhold for mahalanobis\n",
    "    ret,threshold_img = cv2.threshold(scaled_mahalanobis_img,135,255, cv2.THRESH_BINARY_INV)\n",
    "    return threshold_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "def get_tiles(ds, width=WINDOW_WIDTH, height=WINDOW_HEIGTH):\n",
    "    \"\"\"Creating wwindows of a given size for the orthomosaic\n",
    "\n",
    "    Args:\n",
    "        ds (DatasetReader object): Rasterio.open return value for tif file\n",
    "        width (int, optional): Desired window width\n",
    "        height (int, optional): Desired window height\n",
    "        \n",
    "    Yields:\n",
    "        windows.Window: Current window for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting number of coloums and rows and set offsets\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width-OVERLAP_WINDOW), range(0, nrows, height-OVERLAP_WINDOW))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in  offsets:\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_pumpkins_overlap(img: np.ndarray) -> int:\n",
    "    \"\"\"Count the number of pumpkins in the image\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "\n",
    "    Returns:\n",
    "        int: Number of pumpkins\n",
    "    \"\"\"\n",
    "    # Filter\n",
    "    if \"Open\" == FILTER:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        filtered = cv2.erode(img, kernel, iterations=1)\n",
    "        filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "    elif \"Gaussian\" == FILTER:\n",
    "        filtered = cv2.GaussianBlur(img, (3,3), cv2.BORDER_DEFAULT)\n",
    "    elif \"Median\" == FILTER:\n",
    "        filtered = cv2.medianBlur(img, 3)\n",
    "    else:\n",
    "        filtered = img\n",
    "    segmented = segmentation(filtered, method=CieLAB)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    pumpkins:int = 0\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "            #if x < WINDOW_WIDTH-(OVERLAP_WINDOW/2) and y < WINDOW_HEIGTH-(OVERLAP_WINDOW/2):\n",
    "            pumpkins += 1\n",
    "    return pumpkins\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Colour based segmentation\n",
    "This part consists of using colour based information\n",
    "to segment individual images from a pumpkin field.\n",
    "The segmented images should end up having a black\n",
    "background with smaller white objects on top."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.1\n",
    "Annotate some pumpkins in a test image and extract information about the average pumpkin colour\n",
    "in the annotated pixels. Calculate both mean value\n",
    "and standard variation. Use the following two colour\n",
    "spaces: RGB and CieLAB. Finally try to visualise\n",
    "the distribution of colour values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean color values of the annotated pixels:\n",
      " [[ 46.41416428]\n",
      " [150.11533625]\n",
      " [230.19802827]]\n",
      "Standard deviation of color values of the annotated pixels:\n",
      " [[21.71128969]\n",
      " [16.11388805]\n",
      " [17.11028936]]\n",
      "Covariance matrix of color values of the annotated pixels:\n",
      " [[471.40381144 205.71747089 131.0308569 ]\n",
      " [205.71747089 259.6704494  231.02876867]\n",
      " [131.0308569  231.02876867 292.77672851]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_true = cv2.imread(\"Output/1/1/BGR/Ground_true_EB-02-660_0594_0326.JPG\")\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0326.JPG')\n",
    "cv2.imwrite(\"Output/1/1/BGR/img.png\", img)\n",
    "\n",
    "# lower bound and upper bound for red color\n",
    "lower_bound = np.array([0, 0, 250])\n",
    "upper_bound = np.array([0, 0, 255])\n",
    "\n",
    "# Find the colors within the boundaries\n",
    "mask = cv2.inRange(img_true, lower_bound, upper_bound)\n",
    "cv2.imwrite(\"output/1/1/BGR/mask.png\", mask)\n",
    "mask_pixels = np.reshape(mask, (-1))\n",
    "\n",
    "img = cv2.bitwise_and(img, img, mask=mask)\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cv2.imwrite(\"output/1/1/BGR/RGB.png\", img)\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean, std = cv2.meanStdDev(img, mask=mask)\n",
    "print(f\"Mean color values of the annotated pixels:\\n {mean}\")\n",
    "print(f\"Standard deviation of color values of the annotated pixels:\\n {std}\")\n",
    "\n",
    "# Covariance\n",
    "cov = np.cov(pixels.transpose(), aweights=mask_pixels)\n",
    "print(f\"Covariance matrix of color values of the annotated pixels:\\n {cov}\")\n",
    "\n",
    "# Save the parameters\n",
    "np.savetxt(\"Output/1/1/BGR/lower.txt\", mean-std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/BGR/upper.txt\", mean+std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/BGR/mean.txt\", mean, delimiter=' ', fmt='%1.4f')\n",
    "\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "mask = cv2.inRange(img, mean-std, mean+std)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/1/BGR/test.png\", img_test)\n",
    "\n",
    "# Show the histogram\n",
    "colors = ['b', 'g', 'r']\n",
    "for dim in range(img_test.ndim):\n",
    "    data = np.reshape(img_test[:, :, dim], (-1))\n",
    "    plt.hist(data[data != 0], bins=255, color=colors[dim], alpha=0.5)\n",
    "plt.savefig(\"Output/1/1/BGR/histogram.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CieLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean color values of the annotated pixels:\n",
      " [[175.31995372]\n",
      " [150.28308435]\n",
      " [190.01353051]]\n",
      "Standard deviation of color values of the annotated pixels:\n",
      " [[14.39882563]\n",
      " [ 4.32889583]\n",
      " [ 7.2355212 ]]\n",
      "Covariance matrix of color values of the annotated pixels:\n",
      " [[207.33660837 -19.72096074  25.87952376]\n",
      " [-19.72096074  18.74028175   6.27398643]\n",
      " [ 25.87952376   6.27398643  52.35540042]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_true = cv2.imread('Output/1/1/CieLAB/Ground_true_EB-02-660_0594_0326.JPG')\n",
    "img = cv2.imread('Images/EB-02-660_0594_0326.JPG')\n",
    "\n",
    "# lower bound and upper bound for red color\n",
    "lower_bound = np.array([0, 0, 250])\n",
    "upper_bound = np.array([0, 0, 255])\n",
    "\n",
    "# Find the colors within the boundaries\n",
    "mask = cv2.inRange(img_true, lower_bound, upper_bound)\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/mask.png\", mask)\n",
    "mask_pixels = np.reshape(mask, (-1))\n",
    "\n",
    "img = cv2.bitwise_and(img, img, mask=mask)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/CieLAB.png\", img)\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean, std = cv2.meanStdDev(img, mask=mask)\n",
    "print(f\"Mean color values of the annotated pixels:\\n {mean}\")\n",
    "print(f\"Standard deviation of color values of the annotated pixels:\\n {std}\")\n",
    "\n",
    "# Covariance\n",
    "cov = np.cov(pixels.transpose(), aweights=mask_pixels)\n",
    "print(f\"Covariance matrix of color values of the annotated pixels:\\n {cov}\")\n",
    "\n",
    "# Save the parameters\n",
    "np.savetxt(\"Output/1/1/CieLAB/lower.txt\", mean-std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/CieLAB/upper.txt\", mean+std, delimiter=' ', fmt='%1.4f')\n",
    "np.savetxt(\"Output/1/1/CieLAB/mean.txt\", mean, delimiter=' ', fmt='%1.4f')\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "mask = cv2.inRange(img, mean-std, mean+std)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/1/CieLAB/test.png\", img_test)\n",
    "\n",
    "# Show the histogram\n",
    "colors = ['b', 'g', 'r']\n",
    "for dim in range(img_test.ndim):\n",
    "    data = np.reshape(img_test[:, :, dim], (-1))\n",
    "    plt.hist(data[data != 0], bins=255, color=colors[dim], alpha=0.5)\n",
    "plt.savefig(\"Output/1/1/CieLAB/histogram.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.2\n",
    "Segment the orange pumpkins from the background\n",
    "using color information. Experiment with the following segmentation methods\n",
    "1. inRange with RGB values\n",
    "2. inRange with CieLAB values\n",
    "3. Distance in RGB space to a reference colour\n",
    "4. (HLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Test RGB\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "lower_bound = np.loadtxt(\"Output/1/1/BGR/lower.txt\", delimiter=' ')\n",
    "upper_bound = np.loadtxt(\"Output/1/1/BGR/upper.txt\", delimiter=' ')\n",
    "mask = cv2.inRange(img, lower_bound, upper_bound)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/2/BGR.png\", img_test)\n",
    "\n",
    "# Test the output\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "lower_bound = np.loadtxt(f\"Output/1/1/CieLAB/lower.txt\", delimiter=' ')\n",
    "upper_bound = np.loadtxt(f\"Output/1/1/CieLAB/upper.txt\", delimiter=' ')\n",
    "mask = cv2.inRange(img, lower_bound, upper_bound)\n",
    "img_test = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imwrite(\"Output/1/2/CieLAB.png\", img_test)\n",
    "\n",
    "\n",
    "# Distance in RGB space to a reference color\n",
    "ref = np.loadtxt(f\"Output/1/1/BGR/mean.txt\", delimiter=' ')\n",
    "pixels = np.reshape(img, (-1, 3))\n",
    "cov = np.cov(pixels.transpose())\n",
    "diff = pixels - np.repeat([ref], np.ma.size(pixels,axis=0),axis=0)\n",
    "# Calculate the distances\n",
    "mahalanobis = np.sqrt(np.sum(np.dot(diff,np.linalg.inv(cov)) *diff , axis=1))  \n",
    "euclidian = np.sqrt(np.sum(diff * diff, axis=1))\n",
    "# Reshaping to image dimension\n",
    "mahalanobis_img = np.reshape(mahalanobis,(img.shape[0],img.shape[1]))\n",
    "euclidian_img = np.reshape(euclidian,(img.shape[0],img.shape[1]))\n",
    "# Scaling to 8 bit greyscale image\n",
    "scaled_mahalanobis_img = 255*mahalanobis_img/np.max(mahalanobis_img)\n",
    "scaled_euclidian_img = 255*euclidian_img/np.max(euclidian_img)\n",
    "# Saving scaled images\n",
    "cv2.imwrite(\"Output/1/2/euclidian_dist_map.png\", scaled_euclidian_img)\n",
    "cv2.imwrite(\"Output/1/2/mahalanobis_dist_map.png\", scaled_mahalanobis_img)\n",
    "\n",
    "# Setting up a threshhold for mahalanobis\n",
    "ret,threshold_img = cv2.threshold(scaled_mahalanobis_img,135,255, cv2.THRESH_BINARY_INV)\n",
    "threshold_img = threshold_img.astype(np.uint8)\n",
    "cv2.imwrite(\"Output/1/2/mahalanobis_segmented.png\", threshold_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.3\n",
    "Choose one segmentation method to use for the rest\n",
    "of the mini-project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "BGR = \"BGR\"\n",
    "CieLAB = \"CieLAB\"\n",
    "\n",
    "def segmentation(img: np.ndarray, method: str = CieLAB) -> np.ndarray:\n",
    "    \"\"\"Segmentation of an image to find a color spectrum\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "        type (str, optional): Type of image used for segmentation. Defaults to BGR.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image containing the orange color\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the lower and upper bounds\n",
    "    lower_bound = np.loadtxt(f\"Output/1/1/{method}/lower.txt\", delimiter=' ')\n",
    "    upper_bound = np.loadtxt(f\"Output/1/1/{method}/upper.txt\", delimiter=' ')\n",
    "    ref = np.loadtxt(f\"Output/1/1/{method}/mean.txt\", delimiter=' ')\n",
    "    # TODO\n",
    "    ref = np.loadtxt(f\"Output/1/1/BGR/mean.txt\", delimiter=' ')\n",
    "\n",
    "    # Copy the image\n",
    "    img_tmp = img.copy()\n",
    "\n",
    "    if method == BGR:\n",
    "        lower_bound = np.loadtxt(\"Output/1/1/BGR/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(\"Output/1/1/BGR/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "        return img_test\n",
    "    if method == CieLAB:\n",
    "        img_tmp = cv2.cvtColor(img_tmp, cv2.COLOR_BGR2LAB)\n",
    "        lower_bound = np.loadtxt(f\"Output/1/1/CieLAB/lower.txt\", delimiter=' ')\n",
    "        upper_bound = np.loadtxt(f\"Output/1/1/CieLAB/upper.txt\", delimiter=' ')\n",
    "        mask = cv2.inRange(img_tmp, lower_bound, upper_bound)\n",
    "        img_test = cv2.bitwise_and(img_tmp, img_tmp, mask=mask)\n",
    "\n",
    "\n",
    "    # Distance in RGB space to a reference color\n",
    "    pixels = np.reshape(img_tmp, (-1, 3))\n",
    "    cov = np.cov(pixels.transpose())\n",
    "    diff = pixels - np.repeat([ref], np.ma.size(pixels,axis=0),axis=0)\n",
    "    # Calculate the distances\n",
    "    mahalanobis = np.sqrt(np.sum(np.dot(diff,np.linalg.inv(cov)) *diff , axis=1))  \n",
    "    # Reshaping to image dimension\n",
    "    mahalanobis_img = np.reshape(mahalanobis,(img_tmp.shape[0],img_tmp.shape[1]))\n",
    "    # Scaling to 8 bit greyscale image\n",
    "    scaled_mahalanobis_img = 255*mahalanobis_img/np.max(mahalanobis_img)\n",
    "    \n",
    "    # Setting up a threshhold for mahalanobis\n",
    "    ret,threshold_img = cv2.threshold(scaled_mahalanobis_img,135,255, cv2.THRESH_BINARY_INV)\n",
    "    return threshold_img.astype(np.uint8)\n",
    "\n",
    "# Test the segmentation\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "img_test = segmentation(img, method=BGR)\n",
    "cv2.imwrite(\"Output/1/3/BGR.png\", img_test)\n",
    "img_test = segmentation(img, method= CieLAB)\n",
    "cv2.imwrite(\"Output/1/3/CieLAB.png\", img_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Counting objects\n",
    "This part is about counting objects in segmented\n",
    "images and then to generate some visual output that\n",
    "will help you to debug the programs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.1\n",
    "Count the number of orange blobs in the segmented\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number contours: 5378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "segmented = segmentation(img, method=CieLAB)\n",
    "contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Get the width, height and position of each contour\n",
    "width = []\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "        width.append(w)\n",
    "        cv2.circle(img, (x+int(w/2), y+int(h/2)), 10, (0, 0, 255), 2)\n",
    "\n",
    "# Print histogram of size\n",
    "plt.figure()\n",
    "plt.hist(width,bins=255)\n",
    "plt.savefig(\"Output/2/1/histogram.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(f\"Number contours: {len(contours)}\")\n",
    "cv2.imwrite(\"Output/2/1/circle_pumpkins.png\", img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.2\n",
    "Filter the segmented image to remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "segmented = segmentation(img, method=CieLAB)\n",
    "# Opening\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "filtered = cv2.erode(segmented, kernel, iterations=1)\n",
    "filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "cv2.imwrite(\"Output/2/2/filtered.png\", filtered)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.3\n",
    "Count the number of orange blobs in the filtered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number contours: 3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "filtered = cv2.imread('Output/2/2/filtered.png')\n",
    "filtered = cv2.cvtColor(filtered, cv2.COLOR_BGR2GRAY)\n",
    "contours, hierarchy = cv2.findContours(filtered, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Get the width, height and position of each contour\n",
    "width = []\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "        width.append(w)\n",
    "        cv2.circle(img, (x+int(w/2), y+int(h/2)), 10, (0, 0, 255), 2)\n",
    "\n",
    "# Print histogram of size\n",
    "plt.figure()\n",
    "plt.hist(width,bins=255)\n",
    "plt.savefig(\"Output/2/3/histogram.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Number contours: {len(contours)}\")\n",
    "cv2.imwrite(\"Output/2/3/circle_pumpkins.png\", img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.4\n",
    "Mark the located pumpkins in the input image. This\n",
    "step is for debugging purposes and to convince others\n",
    "that you have counted the pumpkins accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Generate an orthomosaic\n",
    "This part deals with orthorectifying multiple images\n",
    "of the same field into a single carthometric product.\n",
    "Choose proper settings for all below processes, taking into consideration the available computing resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.1\n",
    "Load data into Metashape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.2\n",
    "Perform bundle adjustment (align photos) and check\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.3\n",
    "Perform dense reconstruction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.4\n",
    "Create digital elevation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.5\n",
    "Create orthomosaic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.6\n",
    "Limit orthomosaic to pumpkin field"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Count in orthomosaic\n",
    "Use the python package rasterio to perform operations on the orthomosaic using a tile based approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.1\n",
    "Create code that only loads parts of the orthomosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WINDOW_WIDTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m product\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrasterio\u001b[39;00m \u001b[39mimport\u001b[39;00m windows\n\u001b[0;32m----> 4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tiles\u001b[39m(ds, width\u001b[39m=\u001b[39mWINDOW_WIDTH, height\u001b[39m=\u001b[39mWINDOW_HEIGTH):\n\u001b[1;32m      5\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creating wwindows of a given size for the orthomosaic\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m        windows.Window: Current window for processing\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Getting number of coloums and rows and set offsets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WINDOW_WIDTH' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "def get_tiles(ds, width=WINDOW_WIDTH, height=WINDOW_HEIGTH):\n",
    "    \"\"\"Creating wwindows of a given size for the orthomosaic\n",
    "\n",
    "    Args:\n",
    "        ds (DatasetReader object): Rasterio.open return value for tif file\n",
    "        width (int, optional): Desired window width\n",
    "        height (int, optional): Desired window height\n",
    "        \n",
    "    Yields:\n",
    "        windows.Window: Current window for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting number of coloums and rows and set offsets\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width-OVERLAP_WINDOW), range(0, nrows, height-OVERLAP_WINDOW))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in  offsets:\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.2\n",
    "Design tile placement incl. overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.3\n",
    "Count pumpkins in each tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FILTER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m pumpkins\n\u001b[1;32m     34\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mImages/EB-02-660_0594_0344.JPG\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m pumpkins \u001b[39m=\u001b[39m count_pumpkins(img)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber contours: \u001b[39m\u001b[39m{\u001b[39;00mpumpkins\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mcount_pumpkins\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Count the number of pumpkins in the image\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m    int: Number of pumpkins\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Filter\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOpen\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m==\u001b[39m FILTER:\n\u001b[1;32m     15\u001b[0m     kernel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     16\u001b[0m     filtered \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39merode(img, kernel, iterations\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FILTER' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def count_pumpkins(img: np.ndarray) -> int:\n",
    "    \"\"\"Count the number of pumpkins in the image\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "\n",
    "    Returns:\n",
    "        int: Number of pumpkins\n",
    "    \"\"\"\n",
    "    # Filter\n",
    "    if \"Open\" == FILTER:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        filtered = cv2.erode(img, kernel, iterations=1)\n",
    "        filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "    elif \"Gaussian\" == FILTER:\n",
    "        filtered = cv2.GaussianBlur(img, (3,3), cv2.BORDER_DEFAULT)\n",
    "    elif \"Median\" == FILTER:\n",
    "        filtered = cv2.medianBlur(img, 3)\n",
    "    else:\n",
    "        filtered = img\n",
    "    segmented = segmentation(filtered, method=CieLAB)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    pumpkins:int = 0\n",
    "    for c in contours:\n",
    "        _,_,w,h = cv2.boundingRect(c)\n",
    "        if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "            pumpkins += 1\n",
    "    return pumpkins\n",
    "\n",
    "img = cv2.imread('Images/EB-02-660_0594_0344.JPG')\n",
    "pumpkins = count_pumpkins(img)\n",
    "print(f\"Number contours: {pumpkins}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.4\n",
    "Deal with pumpkins in the overlap, so they are only\n",
    "counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def count_pumpkins_overlap(img: np.ndarray) -> int:\n",
    "    \"\"\"Count the number of pumpkins in the image\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Image used for segmentation\n",
    "\n",
    "    Returns:\n",
    "        int: Number of pumpkins\n",
    "    \"\"\"\n",
    "    # Filter\n",
    "    if \"Open\" == FILTER:\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        filtered = cv2.erode(img, kernel, iterations=1)\n",
    "        filtered = cv2.dilate(filtered, kernel, iterations=1)\n",
    "    elif \"Gaussian\" == FILTER:\n",
    "        filtered = cv2.GaussianBlur(img, (3,3), cv2.BORDER_DEFAULT)\n",
    "    elif \"Median\" == FILTER:\n",
    "        filtered = cv2.medianBlur(img, 3)\n",
    "    else:\n",
    "        filtered = img\n",
    "    segmented = segmentation(filtered, method=CieLAB)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(segmented, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    pumpkins:int = 0\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w > MIN_PUMPKIN_WIDTH and h > MIN_PUMPKIN_HEIGHT:\n",
    "            if x < WINDOW_WIDTH-(OVERLAP_WINDOW/2) and y < WINDOW_HEIGTH-(OVERLAP_WINDOW/2):\n",
    "                pumpkins += 1\n",
    "    return pumpkins\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4.5\n",
    "Determine amount of pumpkins in the entire field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2700.66869629  127.69443831 -429.19702376]\n",
      " [ 127.69443831   38.3543187   -31.73406296]\n",
      " [-429.19702376  -31.73406296  108.84516627]]\n",
      "1317\n",
      "[[2122.06247716  156.46030421 -266.4636598 ]\n",
      " [ 156.46030421   68.37073222  -16.91558245]\n",
      " [-266.4636598   -16.91558245  102.03309175]]\n",
      "2665\n",
      "[[2928.84526607  132.10865114 -412.05002365]\n",
      " [ 132.10865114   41.53705054  -32.74189064]\n",
      " [-412.05002365  -32.74189064  103.38738487]]\n",
      "4133\n",
      "[[1439.77078752   87.41398626 -141.61082886]\n",
      " [  87.41398626   53.26390906  -19.54559263]\n",
      " [-141.61082886  -19.54559263   73.207033  ]]\n",
      "4873\n",
      "[[2993.42329757  193.83099231 -454.03657069]\n",
      " [ 193.83099231   60.90889472  -51.72880506]\n",
      " [-454.03657069  -51.72880506  118.66232225]]\n",
      "6213\n",
      "[[1055.9629482    96.90631425 -122.52789339]\n",
      " [  96.90631425   83.39906719  -56.18833667]\n",
      " [-122.52789339  -56.18833667   92.80206014]]\n",
      "6518\n",
      "[[2791.83898555  116.78820356 -375.92690694]\n",
      " [ 116.78820356   42.58853255  -26.93300905]\n",
      " [-375.92690694  -26.93300905   97.04794722]]\n",
      "8117\n",
      "[[2022.20852929   84.70834595 -278.54623848]\n",
      " [  84.70834595   32.17674946  -20.04403389]\n",
      " [-278.54623848  -20.04403389   90.33214698]]\n",
      "8988\n",
      "[[2639.9498289   174.76974215 -440.20402839]\n",
      " [ 174.76974215   46.15000313  -46.57163102]\n",
      " [-440.20402839  -46.57163102  111.11794907]]\n",
      "9948\n",
      "[[1551.79216741  158.45220215 -380.19089755]\n",
      " [ 158.45220215   26.21290845  -45.65981768]\n",
      " [-380.19089755  -45.65981768  108.99464796]]\n",
      "10096\n",
      "[[2014.6804827    64.40028193 -232.09859114]\n",
      " [  64.40028193   47.2879477   -27.45200022]\n",
      " [-232.09859114  -27.45200022   93.66232942]]\n",
      "11669\n",
      "[[2048.86325448   94.56984414 -336.37713504]\n",
      " [  94.56984414   20.73674005  -24.02522034]\n",
      " [-336.37713504  -24.02522034   86.21247522]]\n",
      "12101\n",
      "[[1346.48215947   24.12155374  -76.01661749]\n",
      " [  24.12155374   34.85179309   -6.17201554]\n",
      " [ -76.01661749   -6.17201554   74.50708313]]\n",
      "12921\n",
      "[[1871.35296414   91.87388612 -236.1992018 ]\n",
      " [  91.87388612   10.87242744  -15.34387304]\n",
      " [-236.1992018   -15.34387304   44.60364215]]\n",
      "13054\n",
      "[[1183.92877678   72.74002822 -106.15655275]\n",
      " [  72.74002822   56.02838688  -39.81535034]\n",
      " [-106.15655275  -39.81535034   85.91968699]]\n",
      "13266\n",
      "[[ 719.2420002   102.78930964 -150.23149284]\n",
      " [ 102.78930964   20.90803365  -27.4988292 ]\n",
      " [-150.23149284  -27.4988292    38.87151726]]\n",
      "13268\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import cv2\n",
    "from rasterio.windows import Window\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "filename = \"./Orthomosaic.tif\"\n",
    "\n",
    "with rasterio.open(filename) as src:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for window, transform in get_tiles(src):\n",
    "\n",
    "        img = src.read(window=window)\n",
    "        # Loaded image has a different shape\n",
    "        # than opencv image, so...\n",
    "        temp = img.transpose(1, 2, 0)\n",
    "        t2 = cv2.split(temp)\n",
    "        img_cv = cv2.merge([t2[2], t2[1], t2[0]])\n",
    "\n",
    "        count += count_pumpkins_overlap(img_cv)\n",
    "\n",
    "        print(count)\n",
    "        if count ==5623:\n",
    "                cv2.namedWindow(\"Resized_Window\", cv2.WINDOW_NORMAL)\n",
    "                cv2.resizeWindow(\"Resized_Window\", 300, 700)\n",
    "                cv2.imshow('Resized_Window', img_cv)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Endnotes\n",
    "Reflect on the conducted work in this miniproject."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5.1\n",
    "Determine GSD and size of the image field. What is\n",
    "the average number of pumpkins per area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5.2\n",
    "Reflect on whether the developed system is ready to\n",
    "help a farmer with the task of estimating the number\n",
    "of pumpkins in a field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
